{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries:\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "vgg.to(device)\n",
    "# For non-equivariant\n",
    "for param in vgg.parameters():\n",
    "  param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def load_image(path, max_size=112, shape=(112, 112)):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    # Determine the size to resize the image to\n",
    "    if shape is None:\n",
    "        if max(image.size) > max_size:\n",
    "            size = max_size\n",
    "        else:\n",
    "            size = max(image.size)\n",
    "        size = (size, size)\n",
    "    else:\n",
    "        size = shape\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.CenterCrop((112, 112)),  # Ensure the image is centered and cropped\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "  \"\"\" Display a tensor as an image. \"\"\"\n",
    "  image = tensor.cpu().clone().detach()     # tensor.clone().detach() would create a copy of tensor and removes tensor from computational graph(requires_grad = False)\n",
    "  image = image.numpy().squeeze()\n",
    "  image = image.transpose(1,2,0)\n",
    "  image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "  image = image.clip(0, 1)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model, layers = None):\n",
    "  \"\"\" Run an image forward through a model and get features for a set of layers.\n",
    "      Reference: Gatys et al (2016)\n",
    "  \"\"\"\n",
    "  #'conv4_2' below is used for content representation\n",
    "  if layers is None:\n",
    "    layers = {'0' : 'conv1_1',\n",
    "              '5' : 'conv2_1',\n",
    "              '10' : 'conv3_1',\n",
    "              '19': 'conv4_1',\n",
    "              '21': 'conv4_2',\n",
    "              '28': 'conv5_1'}\n",
    "            \n",
    "  features = {}\n",
    "  x = image\n",
    "  for name, layer in model._modules.items():\n",
    "    x = layer(x)   #passing image through layer\n",
    "    if name in layers:\n",
    "      features[layers[name]] = x\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram Matrix Definitions\n",
    "def gram_matrix(tensor):\n",
    "  # get batch_size, depth, height, width of tensor\n",
    "  _, d, h, w = tensor.size()\n",
    "  # reshape so we are multiplying height and width\n",
    "  tensor = tensor.view(d, h * w)\n",
    "  # calc. gram matrix\n",
    "  gram = torch.mm(tensor, tensor.t())\n",
    "\n",
    "  return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_style(show_every=250, \n",
    "                   optimizer_lr=0.005, \n",
    "                   tot_steps=2000, \n",
    "                   content_alpha=1, \n",
    "                   style_beta=1e7, \n",
    "                   content_layer='conv4_2', \n",
    "                   style_weights={'conv1_1': 0.2, 'conv2_1': 0.2, 'conv3_1': 0.2, 'conv4_1': 0.2, 'conv5_1': 0.2}, \n",
    "                   content_pth=None, \n",
    "                   style_pth=None, \n",
    "                   l2_loss=True,\n",
    "                   gram_function=gram_matrix):\n",
    "    \n",
    "    if content_pth is None or style_pth is None:\n",
    "        raise ValueError(\"Both content_pth and style_pth must be provided\")\n",
    "    \n",
    "    gram_tensors = []  # List to store Gram matrices with their layer and step\n",
    "\n",
    "    # Load content and style images\n",
    "    content = load_image(content_pth).to(device)\n",
    "    style = load_image(style_pth, shape=(content.shape[2], content.shape[3])).to(device)\n",
    "    \n",
    "    # Initialize the target image\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "    plt.imshow(im_convert(target))\n",
    "    plt.show()\n",
    "\n",
    "    # Get features from the content and style images\n",
    "    content_layers = {'21': 'conv4_2'}\n",
    "    content_features = get_features(content, vgg, layers=content_layers)\n",
    "    \n",
    "    # Display the content feature at conv4_2\n",
    "    if 'conv4_2' in content_features:\n",
    "        print(\"Content feature at conv4_2\")\n",
    "        plt.imshow(im_convert(content_features['conv4_2'][0]))  # Ensure batch dimension is removed\n",
    "        plt.title(\"Content feature at conv4_2\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Display the style features\n",
    "    style_features = get_features(style, vgg)\n",
    "    for layer, feature in style_features.items():\n",
    "        print(f\"Style feature at {layer}\")\n",
    "        plt.imshow(im_convert(feature[0]))\n",
    "        plt.title(f\"Style feature at {layer}\")\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate gram matrices for each layer of our style representation\n",
    "    style_grams = {layer: gram_function(style_features[layer]) for layer in style_features}\n",
    "\n",
    "    # Iteration hyperparameters\n",
    "    optimizer = optim.Adam([target], lr=optimizer_lr)\n",
    "    #optimizer = optim.SGD([target], lr=0.01, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    num_layers = len(style_weights)\n",
    "    print(f\"Number of style layers: {num_layers}\")\n",
    "\n",
    "    for ii in range(1, tot_steps + 1):\n",
    "        # Get the features from your target image\n",
    "        target_features = get_features(target, vgg)\n",
    "\n",
    "        # Compute the content loss\n",
    "        if l2_loss == True:\n",
    "            content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "        else: \n",
    "            content_loss = torch.mean(torch.abs(target_features[content_layer] - content_features[content_layer]))\n",
    "\n",
    "        # Initialize the style loss\n",
    "        style_loss = 0\n",
    "\n",
    "        # Compute the style loss for each layer\n",
    "        for layer in style_weights:\n",
    "            # Get the \"target\" style representation for the layer\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_function(target_feature)\n",
    "\n",
    "            # Append the step, layer, and computed Gram matrix to the list\n",
    "            gram_tensors.append((ii, layer, target_gram.detach()))\n",
    "\n",
    "            _, d, h, w = target_feature.shape\n",
    "            \n",
    "            style_gram = style_grams[layer]\n",
    "            \n",
    "            # Compute the style loss for one layer, weighted appropriately\n",
    "            if l2_loss == True:\n",
    "                layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            else: \n",
    "                layer_style_loss = style_weights[layer] * torch.mean(torch.abs(target_gram - style_gram))\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "        # Calculate the total loss\n",
    "        total_loss = content_alpha * content_loss + style_beta * style_loss\n",
    "\n",
    "        # Update the target image\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if ii % show_every == 0:\n",
    "            print('Total loss: ', total_loss.item())\n",
    "            plt.imshow(im_convert(target))\n",
    "            plt.show()\n",
    "\n",
    "    expected_length = tot_steps * num_layers\n",
    "    print(f\"Expected length of gram_tensors: {expected_length}\")\n",
    "\n",
    "    return gram_tensors  # Return the list of Gram matrices with their corresponding step and layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gram_matrices(gram_tensors, \n",
    "                            filter_layer=None, \n",
    "                            step_interval=100, \n",
    "                            content=\"content\", \n",
    "                            style=\"style\", \n",
    "                            symmetries=\"symmetries\"):\n",
    "    # Create a list of filtered Gram matrices based on the filter_layer and step_interval\n",
    "    filtered_grams = [(step, layer, gram_matrix) for step, layer, gram_matrix in gram_tensors if (not filter_layer or layer == filter_layer) and step % step_interval == 0]\n",
    "\n",
    "    # Calculate the number of subplots needed\n",
    "    num_subplots = len(filtered_grams)\n",
    "    cols = 2  # Number of columns in the subplot grid\n",
    "    rows = (num_subplots + cols - 1) // cols  # Calculate the number of rows needed\n",
    "    \n",
    "    # Create a larger figure for the subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "    \n",
    "    for idx, (step, layer, gram_matrix) in enumerate(filtered_grams):\n",
    "        ax = axes[idx]\n",
    "        gram_matrix_np = gram_matrix.cpu().detach().numpy()  # Ensure the tensor is detached and moved to CPU\n",
    "        sns.heatmap(gram_matrix_np, cmap='viridis', ax=ax, cbar=True)\n",
    "        ax.set_title(f'Gram Matrix at Step {step}, Layer {layer}')\n",
    "        ax.set_xlabel('Feature Maps')\n",
    "        ax.set_ylabel('Feature Maps')\n",
    "    \n",
    "    # Remove any empty subplots\n",
    "    for idx in range(len(filtered_grams), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    # Adjust layout and save the figure as a PDF\n",
    "    fig.tight_layout()\n",
    "    pdf_title = f\"{content} {style} {symmetries}.pdf\"\n",
    "    plt.savefig(pdf_title)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
