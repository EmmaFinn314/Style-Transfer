{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Diheadral Group of Order 8\n",
    "class D8Group:\n",
    "    def __init__(self):\n",
    "        self.elements = self._generate_elements()\n",
    "\n",
    "    def _generate_elements(self):\n",
    "        elements = []\n",
    "        for r in range(4):\n",
    "            for f in range(2):\n",
    "                elements.append((r, f))\n",
    "        return elements\n",
    "\n",
    "    def order(self):\n",
    "        return len(self.elements)\n",
    "\n",
    "    def apply(self, element, x):\n",
    "        r, f = element\n",
    "        if f == 1:\n",
    "            x = torch.flip(x, [-1])\n",
    "        x = torch.rot90(x, r, [-2, -1])\n",
    "        return x\n",
    "\n",
    "    def inverse(self, element):\n",
    "        r, f = element\n",
    "        return (4 - r) % 4, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Definitions\n",
    "\n",
    "class EquivariantMaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None, padding=0):\n",
    "        super(EquivariantMaxPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.D8 = D8Group()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for element in self.D8.elements:\n",
    "            transformed_input = self.D8.apply(element, x)\n",
    "            pooled_output = F.max_pool2d(transformed_input, self.kernel_size, self.stride, self.padding)\n",
    "            out.append(self.D8.apply(self.D8.inverse(element), pooled_output))\n",
    "        out = torch.stack(out, dim=1)\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "\n",
    "class EquivariantConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(EquivariantConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.D8 = D8Group()\n",
    "        self.weights = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for element in self.D8.elements:\n",
    "            transformed_weight = self.D8.apply(element, self.weights)\n",
    "            out.append(F.conv2d(x, transformed_weight, stride=self.stride, padding=self.padding))\n",
    "        out = torch.stack(out, dim=1)\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "\n",
    "class EquivariantVGG19(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(EquivariantVGG19, self).__init__()\n",
    "        self.features = self._make_layers()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def _make_layers(self):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [EquivariantMaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [EquivariantConv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def check_layer_equivariance(layer, input_tensor, tolerance=1e-3):\n",
    "    d8 = D8Group()\n",
    "    for element in d8.elements:\n",
    "        transformed_input = d8.apply(element, input_tensor)\n",
    "        original_output = layer(input_tensor)\n",
    "        transformed_output = layer(transformed_input)\n",
    "        expected_transformed_output = d8.apply(element, original_output)\n",
    "        \n",
    "        print(f\"Element: {element}\")\n",
    "        \n",
    "        if not torch.allclose(transformed_output, expected_transformed_output, atol=tolerance):\n",
    "            print(f\"Layer equivariance test failed for element: {element}\")\n",
    "            print(f\"Transformed Input Shape: {transformed_input.shape}\")\n",
    "            print(f\"Original Output Shape: {original_output.shape}\")\n",
    "            print(f\"Transformed Output Shape: {transformed_output.shape}\")\n",
    "            print(f\"Expected Transformed Output Shape: {expected_transformed_output.shape}\")\n",
    "            print(\"Difference:\")\n",
    "            print(torch.mean(torch.abs(transformed_output - expected_transformed_output)))\n",
    "            return False\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
