{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_eq(image, model, layers):\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model.features._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "d8_vgg = EquivariantVGG19(num_classes = 100)\n",
    "d8_vgg = d8_vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivariant Style Transfer\n",
    "def transfer_style_eq(model=None,\n",
    "                   show_every=250, \n",
    "                   optimizer_lr=0.005, \n",
    "                   tot_steps=2000, \n",
    "                   content_alpha=1, \n",
    "                   style_beta=1e7, \n",
    "                   content_layers={21: 'conv4_2'}, \n",
    "                   style_weights={'conv1_1': 0.2, 'conv2_1': 0.2, 'conv3_1': 0.2, 'conv4_1': 0.2, 'conv5_1': 0.2}, \n",
    "                   content_pth=None, \n",
    "                   style_pth=None, \n",
    "                   l2_loss=True,\n",
    "                   gram_function=gram_matrix):\n",
    "    \n",
    "    if content_pth is None or style_pth is None:\n",
    "        raise ValueError(\"Both content_pth and style_pth must be provided\")\n",
    "    \n",
    "    gram_tensors = []  # List to store Gram matrices with their layer and step\n",
    "\n",
    "    # Load content and style images\n",
    "    content = load_image(content_pth, 224).to(device)\n",
    "    style = load_image(style_pth, 224).to(device)\n",
    "    \n",
    "    # Initialize the target image\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "    plt.imshow(im_convert(target))\n",
    "    plt.show()\n",
    "\n",
    "    # Get features from the content and style images\n",
    "    content_features = get_features_eq(content, model, layers=content_layers)\n",
    "    \n",
    "    # Calculate gram matrices for each layer of our style representation\n",
    "    style_layers = {layer: layer for layer in style_weights}\n",
    "    style_features = get_features_eq(style, model, layers=style_layers)\n",
    "    style_grams = {layer: gram_function(style_features[layer]) for layer in style_features}\n",
    "\n",
    "    # Iteration hyperparameters\n",
    "    optimizer = optim.Adam([target], lr=optimizer_lr)\n",
    "    \n",
    "    num_layers = len(style_weights)\n",
    "    print(f\"Number of style layers: {num_layers}\")\n",
    "\n",
    "    for ii in range(1, tot_steps + 1):\n",
    "        # Get the features from your target image\n",
    "        target_features = get_features_eq(target, model, layers={**content_layers, **style_layers})\n",
    "        \n",
    "        # Debug statement to print the target feature keys\n",
    "        if ii == 1:\n",
    "            print(\"Target feature keys:\", target_features.keys())\n",
    "\n",
    "        # Compute the content loss\n",
    "        content_layer = list(content_layers.values())[0]  # Get the content layer key\n",
    "        if l2_loss:\n",
    "            content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "        else: \n",
    "            content_loss = torch.mean(torch.abs(target_features[content_layer] - content_features[content_layer]))\n",
    "\n",
    "        # Initialize the style loss\n",
    "        style_loss = 0\n",
    "\n",
    "        # Compute the style loss for each layer\n",
    "        for layer in style_weights:\n",
    "            # Get the \"target\" style representation for the layer\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_function(target_feature)\n",
    "\n",
    "            # Append the step, layer, and computed Gram matrix to the list\n",
    "            gram_tensors.append((ii, layer, target_gram.detach()))\n",
    "\n",
    "            _, d, h, w = target_feature.shape\n",
    "            \n",
    "            style_gram = style_grams[layer]\n",
    "            \n",
    "            # Compute the style loss for one layer, weighted appropriately\n",
    "            if l2_loss:\n",
    "                layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            else: \n",
    "                layer_style_loss = style_weights[layer] * torch.mean(torch.abs(target_gram - style_gram))\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "        # Calculate the total loss\n",
    "        total_loss = content_alpha * content_loss + style_beta * style_loss\n",
    "\n",
    "        # Update the target image\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward(retain_graph = True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if ii % show_every == 0:\n",
    "            print('Total loss: ', total_loss.item())\n",
    "            plt.imshow(im_convert(target))\n",
    "            plt.show()\n",
    "\n",
    "    expected_length = tot_steps * num_layers\n",
    "    print(f\"Expected length of gram_tensors: {expected_length}\")\n",
    "\n",
    "    return gram_tensors"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
